\section{Related Work}
\label{sec:relatedwork}


%Unified VM is either supported with conventional address translation~\cite{pugsley:ndc, gao:practical} or deferring translation to the CPU cores or IOMMUs~\cite{xi:beyond}.

%namely the scan operator with GPUs~\cite{power:implications} and custom logic~\cite{xi:beyond}, and the join operator with microcontrollers~\cite{mirzadeh:sort}. 

%Tesseract relies on MPI without virtual memory. NDC and NDP assume a global address space with conventional address translation. Ahn et al.~\cite{ahn:pim-enabled} execute some ISA instructions of the host next to the memory, while translation is performed on the CPU.\\

%\noindent\textbf{Unified virtual memory} Industry and academia have been recently pushing towards a unified virtual memory between CPUs and GPUs. Industry examples include the HSA Foundation~\cite{hsa} and Nvidia's Unified Memory~\cite{harris:unified}. Academic publications~\cite{power:supporting, pichai:architectural} propose a TLB architecture to sustain the high translation throughput required for GPU cores. A recent study on IOMMU translation for integrated GPUs has shown that a TLB miss takes an order of magnitude longer than on CPU cores~\cite{vesely:observation}. 

%Several studies exploited the contiguity generated by using the buddy allocator and memory compactor to increase the TLB reach. 

%CoLT \cite{pham:colt}, Clustered TLBs\cite{pham:increasing}, and sub-blocked TLBs\cite{talluri:surpassing} group multiple PTEs into a single TLB entry. Direct segments~\cite{basu:efficient} allows for an efficient mapping between a single virtual segment mapped contiguously in physical memory. Karakostas et al.~\cite{karakostas:redundant} propose a fully associative range-based TLB and page table to transparently exploit the available contiguity in the physical memory. Transparent Huge Pages~\cite{transparenthugepages} and libHugeTLBFS~\cite{lighugetlbfs} increase the TLB reach by mapping large regions to a single TLB entry.

\noindent\textbf{Improving TLB performance.} Techniques described in Section~\ref{sec:background} improve TLB reach. Other techniques target reducing TLB miss penalty. Commercial processors store page table entries in data caches to accelerate page walks~\cite{intel:architectures}. Some architectures use page table caches (e.g., TSBs in SPARC~\cite{sun:ultrasparc}). MMU caches are employed to skip walking intermediate page table levels~\cite{bhattacharjee:large-reach, barr:translation}. Other techniques translate speculatively~\cite{barr:spectlb} or prefetch TLB entries~\cite{bhattacharjee:characterizing}. LegoOS~\cite{shan:legoos} recently argued that address translation should be delegated to memory.

%~\cite{pham:colt, pham:increasing, basu:efficient, karakostas:redundant, transparenthugepages, lighugetlbfs, park:hybrid}

\noindent\textbf{Reducing VM associativity.} We are not the first to restrict VM associativity. Several degrees of page coloring---fixing a few bits from the virtual-to-physical map---were proposed in the past. MIPS R6000 used page coloring coupled with a small TLB to index the cache under tight latency constraints~\cite{taylor:tlb}. Page coloring has also been used for virtually indexed physically tagged caches~\cite{chiueh:eliminating} as an alternative to large associativities~\cite{gustafson:ibm} or page sizes~\cite{jouppi:architectural}. Alan Jay Smith~\cite{smith:comparative} advocated the usage of set-associative mappings for main memory to simplify page placement and replacement. 

%---much like another cache level---to 

%\javier{Should I remove from the above how translation is performed? Then I talk about it below.} \\
%\javier{Talk about DNNs on 3D memories}\\
%\javier{Add HPCA paper on virtual memory for accelerators} \\
%\javier{Rewrite it to talk about unified virtual memory.}\\

%\noindent\textbf{Page Tables.} Hierarchical, linear, inverted. Software caches for page tables. \javier{Talk about page table caches (e.g., TSB)}\\

