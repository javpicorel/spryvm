\section{Discussion \& Future Work}
\label{sec:discussion}

Following the prior work, we assumed that the accelerators do not have caches~\cite{haria:devirtualizing, picorel:near-memory}, as they are usually tailored for acceleration of data traversals that are not cache friendly. However, DTRIM is equally applicable to systems with caches, as well as the systems with execution-side TLBs and MMUs. In such cases, DTRIM would be very useful in accelerating the handling of TLB misses. Recent practical designs for virtual cache hierarchies~\cite{yoon:revisiting, park:efficient} would also be a great fit for DTRIM, obviating the need for execution-side TLB hardware. Also note that DTRIM could be used for accelerating CPU TLB misses, which we plan to explore in the future. 

DTRIM's memory-side translation hardware could be looked at as a fully distributed IOMMUs. The key difference is that IOMMUs serve translation/TLB miss requests for the devices attached to the local socket, but in doing so they cache translations and perform page walks across the entire memory system, while DTRIM's translations and page walks are confined within the local memory partition, which is a key to scalability. 

%\noindent\textbf{Page faults.} Upon a page fault triggered by an MPU, we choose to interrupt the CPU to run a handler, as MPUs may not be capable of running an OS. The memory-side MMU notifies the MPU of the fault, and then the MPU places a request in a memory-mapped queue indicating the faulting virtual address and MPU's id. Then, it raises an interrupt on the CPU. The handler running on the CPU resolves the page fault and updates both the CPU's and memory-side MMU's page table. All MMU state is exposed through memory-mapped IO with an uncacheable memory policy. Once the fault is serviced, the handler notifies the appropriate MMU, which resumes execution and retries the faulting address. Such page fault processing is also employed in today's integrated GPUs~\cite{vesely:observation}.\\

%\noindent\textbf{TLB shootdowns.} Many ways exist to maintain the memory-side page tables coherent upon TLB shootdowns initiated by the CPU.  Our approach is similar to those used in integrated GPUs: an OS driver monitors any changes on virtual address spaces shared with MPUs, triggering update operations for the affected page table entries. Note that the inverted nature of the page tables eliminates any global coherence activity in the memory network as a virtual page maps to only one partition, and consequently, page table. Additionally, modifying the desired page table entry requires a single memory access in the common case, accounting for at most a few hundred nanoseconds. This additional overhead is not as significant as the TLB shootdown operation itself, which removes the stale entries in the TLBs of MPUs and CPU cores, and already takes tens of microseconds~\cite{oskin:software-managed}.\\

%\mjs{does this imply that all MPU translations are snooped into the CPU? discuss scalability?}

%\noindent\textbf{Cache hierarchy.} We assume that accelerators, much like conventional cores, could integrate te physical caches and an execution-side MMU with TLBs. Upon a page walk, the memory-side MMUs reply with the page table entry and cache block, which are stored in the MMU's TLB and data cache respectively. However, to avoid TLBs and TLB shootdowns~\cite{villavieja:didi}, we could use virtual caches. Recent practical designs for virtual cache hierarchies~\cite{yoon:revisiting, park:efficient} would be a perfect fit for DTRIM. In this approach, MPUs access the cache with virtual addresses, and upon a cache miss, the request is propagated to the memory-side MMUs to translate and fetch the corresponding block. The memory-side MMU only replies with the data cache block, simplifying our design.     

%\noindent\textbf{Synonyms.} In our current design, all the synonyms of a particular page frame need to map to the same memory set or partition. We believe this is not a significant limitation as it has been already included in commercial systems~\cite{cheng:virtual}. Nevertheless, our page table entries contain the whole page frame address, and hence with a slight modification in our design, we could enable synonyms to map anywhere. The only potential consequence is a drop in performance as a synonym page might map to a page frame residing in another memory partition. However, prior work has already shown that synonyms are both scarce and infrequent~\cite{yoon:revisiting, basu:reducing, park:efficient}, and therefore this overhead should be negligible.\\

%\noindent\textbf{DIPTA for CPUs.} DIPTA is a near-memory structure independent of any control flow, and any execution unit can use it to eliminate the translation overhead for the part of memory covered by DIPTA. While the focus of this work is on MPUs, CPUs can use DIPTA the same way MPUs do. However, CPUs may have to continue using TLBs in order to efficiently address any planar DDR chips and maintain full flexibility.\\

%\noindent\textbf{Multi-level memories.} Though prior work on MPUs assumes a single level~\cite{gao:practical, ahn:scalable, pugsley:ndc, ahn:pim-enabled}, memory can be organized as a hierarchy, with a die-stacked cache~\cite{reinders:knights, volos:fat} backed up by planar memory. For hardware-managed caches, the memory-side MMU performs the translation and accesses the partition, and in case of a cache miss, the page is fetched from planar memory as part of the standard cache miss operation. Once the page arrives into the partition, the data is sent back to the MPU. Note that moving the page from planar memory to the 3D memory does not affect the page table entry. In software-managed caches~\cite{reinders:knights}, MPUs rely on a software API for explicit migration of pages into the die-stacked memories.

%, as MPUs cannot access planar memory directly. 

%\noindent\textbf{Kernel memory.} We consider user instructions only as we argue that MPUs, like all prior custom hardware (e.g., GPU, FPGA), should execute only user code. Nevertheless, DTRIM is a great fit for the kernel, as Linux and FreeBSD's memory usage is almost entirely direct-mapped~\cite{mauerer:professional, mckusick:design} and memory resident. The TLB's tag matching logic would only require to skip the ASID bits for kernel accesses.

%As all the processes share the same kernel virtual addresses, the logic needs to ignore the ASID bits for kernel accesses. The implementation is trivial as Linux and FreeBSD partition the address space into two halves.

%, and hence the virtual address dictates whether to ignore the ASID bits.

%\mjs{does this imply that you want to run the kernel in the MPUs? i'm nto sure why this is an issue for your technique if only the CPU runs the kernel, all of its translations will just stay local and the MPUs will run their user-level code happily in the distance}

%\noindent\textbf{OS support.} The OS only needs to guarantee that the virtual page number and the page frame number map to the same memory set. OSs that support virtual caches already provide this capability (e.g., Solaris~\cite{cheng:virtual} and MIPS OS~\cite{taylor:tlb}). Therefore, the required modifications to OSs that do not have this feature should be feasible.


