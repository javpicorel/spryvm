\section{Experimental Methodology}
\label{sec:methodology}

\begin{table}
        \begin{center}
                \caption{Server Workload description.}
                \scalebox{0.7}
                \small
                \vspace{0.01in}
                \label{table:workload}
                \renewcommand{\arraystretch}{1.0}
                {\scriptsize
                        \begin{tabular}{ l  l }
                                \toprule
                                {\bf Workload}                  & {\bf Description}  \\
                                \toprule
                                \multirow{1}{*}{Cassandra}      &  NoSQL data store running Yahoo's YCSB. \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{Memcached}      & Cache store running Twitter-like workload~\cite{lim:thin}. \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{TPC-H}          & TPC-H on MonetDB column store (Q1-Q21). \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{TPC-DS}         & TPC-DS on MonetDB column store (Queries of~\cite{kocberber:meet}). \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{MySQL}          & SQL DBMS running Facebook's LinkBench~\cite{facebook:linkbench}. \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{RocksDB}        &  Store engine running Facebook benchmarks~\cite{facebook:rocksdb}. \\ %a canonical uniform request distribution. \\

                                \bottomrule
                        \end{tabular}
                } %small
        \end{center}
        \vspace{-0.1in}
\end{table}

Like most recent work on VM~\cite{basu:efficient, pham:increasing, pham:colt, bhattacharjee:large-reach, barr:spectlb, papadopoulou:prediction-based, saulsbury:recently-based}, we use a combination of real-hardware measurements, trace-driven functional simulation, analytical models, and Linux kernel prototypes.

\subsection{Workloads}
To study the impact of VM associativity on general-purpose software, we use a suite of popular server workloads, shown in Table~\ref{table:workload}, in a variety of setups. First, we use them to study the very limits of reducing associativity (which determines the limits on memory partitioning) using a 3C page fault model and Pin traces. For practical reasons these workloads are tuned to fit in 8GB memory for this experiment. Then we study the behavior of the server workloads on real hardware with our modified linux kernel (v4.10) with set-associative memory (which we plan to open-source), using more realistic dataset sizes (16-32GB), in both in-memory and out-of-memory setups.

To study the TLB behavior at very large datasets (128GB), we use  data traversal applications present in the ASCYLIB~\cite{david:asynchronized} suite, which contains state-of-the-art implementations of hash tables, external and internal binary trees, and skip lists, which are the core of many of the server workloads, such as Memcached and RocksDB. We choose these workloads because of their minimal data locality and instruction-level parallelism, and the fact that they are stressing conventional general-purpose CPU architectures and hence favor custom memory-side hardware~\cite{ haria:devirtualizing, picorel:near-memory, kocberber:meet, hsieh:accelerating}. Similar to prior work~\cite{picorel:near-memory}, we present the results for four representative implementations for space reasons. 

%: Java Hash Table (Hash Table), Fraser Skip List (Skip List), Howley Binary Search Tree (BST Internal), and Natarajan Binary Search Tree (BST External).

%Furthermore, following the premises of a unified VM and enabling "pointer-is-a-pointer" semantics, it is not expected that accelerators will run an end-to-end workload in isolation. In contrast, the expected model is to offload certain parts of the CPU's execution which are more efficiently executed on the accelerators. Although the offloading mechanism is out of the scope of this paper, we decided to stress the notion of associativity in VM with a set of popular server workload, shown in Table~\ref{table:workload}. The reason is that these workloads stress the VM mechanism, with large memory requirements, often allocation and de-allocation of memory, and overly fragmented virtual memory layouts~\cite{picorel:near-memory}. Moreover, the evaluated data structure traversals are in fact the core of many of the server workloads, such as Memcached's hash table and RocksDB' skip list data structures.

\subsection{Limits to Partitioning}
%As our approach intends to find a middle-ground between fully associative (i.e., conventional translation) and direct mapping~\cite{picorel:near-memory, haria:devirtualizing}, we aim to fill the research void of what is the level of associativity needed. 
For a system with $N$ physical memory pages, increasing the number of memory partitions from 1 to $p$ reduces the VM associativity from $N$ (fully associative) to $N/p$. Therefore, to determine the maximum number of memory partitions, we look at the minimum associativity requirements of the virtual-to-physical address mapping. Interestingly, there exists no study on VM associativity, unlike caches, which are similar in some organizational aspects and for which such a study has existed for three decades~\cite{hill:aspects}. To study the VM associativity, we employ a 3C page fault model, analogous to the well-established 3C cache miss model~\cite{hill:aspects}. In this context, associativity means the number of candidate page frames a given virtual page can map to. This model classifies misses (i.e., page faults) into three categories: conflict misses (when too many active pages map to a fraction of the sets), capacity misses (due to limited memory size), and compulsory misses (upon the first access to a page). 

We perform this study on modern server workloads. For workloads with fine-grained operations (i.e., Memcached, RocksDB, MySQL, and Cassandra), we collect memory traces containing the same number of instructions as the application executes in 60 seconds without Pin. For analytics workloads (i.e., TPC-H \& TPC-DS), we instrument the entire execution. We extract the ASID bits and virtual address of each memory access, concatenate both~\cite{basu:reducing, yoon:revisiting},  and use it to probe a set-associative memory structure. For the associativity experiments, we tune the workloads so that the aggregate resident set size (RSS) of all running processes is 8GB; for single-process runs, the process has an RSS of 8GBs; for two-process runs, each process has an RSS of 4GB,  etc. To vary the memory size to working set size ratio, we vary the size of the set-associative memory structure. 

We collect the traces on a dual-socket server CPU (Intel Xeon E5-2680 v3) with $256$GB of memory, using the Linux 3.10 kernel and Google's TCMalloc~\cite{google:tcmalloc}. Address space randomization (ASLR) is enabled in all experiments.

\subsection{TLB studies}
To study the impact of memory size on  TLB performance (Figure~\ref{fig:pagewalks}), we perform measurements on real hardware using the \textit{perf} tool during a 10min execution window. To study the TLB capacity requirements of different designs, we collect memory traces for 128GB datasets using Pin~\cite{luk:pin}, with each trace containing 1B instructions, which is more than enough for the TLB sensitivity experiments up to a few thousand entries. We use the traces to probe a set-associative TLB structure while varying its size and organization, after validating the baseline TLB model against the measurements on real hardware. 

\subsection{Performance}
Full-system simulation for TLB misses and page faults is not practical, as these events occur less frequently than other micro-architectural events (e.g., branch mispredictions). Hence, we resort to the CPI models often used in VM research~\cite{papadopoulou:prediction-based, saulsbury:recently-based, bhattacharjee:shared} to sketch the performance gains. These prior studies report performance as the reduction in the translation-related cycles per instruction. As CPI components are additive, this metric is valid irrespective of the workload's baseline CPI. The CPI is measured by applying fixed penalties to the TLB miss rates obtained with the Pin traces. 
% We further strengthen this methodology by studying the CPI savings on all memory operations, not only on translation (as we overlap translation and data fetch operations). Our model thus captures both the translation and data fetch cycles, which together constitute the largest fraction of the total CPI in data structure traversals~\cite{picorel:near-memory}. 



