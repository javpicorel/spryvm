\section{Experimental Methodology}
\label{sec:methodology}

\begin{table}
        \begin{center}
                \caption{Server Workload description.}
                \scalebox{0.7}
                \small
                \vspace{0.01in}
                \label{table:workload}
                \renewcommand{\arraystretch}{1.0}
                {\scriptsize
                        \begin{tabular}{ l  l }
                                \toprule
                                {\bf Workload}                  & {\bf Description}  \\
                                \toprule
                                \multirow{1}{*}{Cassandra}      &  NoSQL data store running Yahoo's YCSB. \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{Memcached}      & Cache store running Twitter-like workload~\cite{lim:thin}. \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{TPC-H}          & TPC-H on MonetDB column store (Q1-Q21). \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{TPC-DS}         & TPC-DS on MonetDB column store (Queries of~\cite{kocberber:meet}). \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{MySQL}          & SQL DBMS running Facebook's LinkBench~\cite{facebook:linkbench}. \\
                                \cmidrule{2-2}
                                \multirow{1}{*}{RocksDB}        &  Store engine running Facebook benchmarks~\cite{facebook:rocksdb}. \\ %a canonical uniform request distribution. \\

                                \bottomrule
                        \end{tabular}
                } %small
        \end{center}
        \vspace{-0.1in}
\end{table}

Like most recent work on VM~\cite{basu:efficient, pham:increasing, pham:colt, bhattacharjee:large-reach, barr:spectlb, papadopoulou:prediction-based, saulsbury:recently-based}, we use a combination of real-hardware measurements, trace-driven functional simulation, analytical models, and linux kernel prototypes.

\subsection{Workloads}

To study the impact of associativity on general-purpose software, we use a suite of popular server workloads, shown in Table~\ref{table:workload}, in a variety of setups. First, we use them to study the very limits of reducing associativity using a 3C page fault model and Pin traces. For practical reasons these workloads are tuned to fit in 8GB memory for this experiment. Then we study the behavior of the server workloads on real hardware with our modified linux kernel (v4.10) with set-associative memory (which we plan to open-source), using more realistic dataset sizes (16-32GB), in both in-memory and out-of-memory setups.

To study the TLB behavior at very large datasets (128GB), we use  data traversal applications present in the ASCYLIB~\cite{david:asynchronized} suite, which contains state-of-the-art implementations of hash tables, extarnal and internal binary trees, and skip lists, which are the core of many of the server workloads, such as Memcached and RocksDB. We choose these workloads because of their minimal data locality and instruction-level parallelism, and the fact that they are stressing conventional general-purpose CPU architectures and hence favor custom hardware~\cite{ haria:devirtualizing, picorel:near-memory, kocberber:meet, hsieh:accelerating}. Similar to prior work~\cite{picorel:near-memory}, we present the results for four representative implementations for space reasons. 

%: Java Hash Table (Hash Table), Fraser Skip List (Skip List), Howley Binary Search Tree (BST Internal), and Natarajan Binary Search Tree (BST External).

%Furthermore, following the premises of a unified VM and enabling "pointer-is-a-pointer" semantics, it is not expected that accelerators will run an end-to-end workload in isolation. In contrast, the expected model is to offload certain parts of the CPU's execution which are more efficiently executed on the accelerators. Although the offloading mechanism is out of the scope of this paper, we decided to stress the notion of associativity in VM with a set of popular server workload, shown in Table~\ref{table:workload}. The reason is that these workloads stress the VM mechanism, with large memory requirements, often allocation and de-allocation of memory, and overly fragmented virtual memory layouts~\cite{picorel:near-memory}. Moreover, the evaluated data structure traversals are in fact the core of many of the server workloads, such as Memcached's hash table and RocksDB' skip list data structures.

\subsection{TLB studies}
To study the impact on memory size on  TLB performance (Figure~\ref{fig:pagewalks}), we perform measurements on real hardware using the \textit{perf} tool during a 10min execution window. To study the impact of TLB size, we collect memory traces for 32GB and 128GB datasets using Pin~\cite{luk:pin}, with each trace containing 1B instrutions, which is more than enough for the TLB sensitivity experiments up to a few thousand entries. We use the traces to probe a set-associative TLB structure while varying its size and organization, after validating the baseline TLB model against the measurements on real hardware. 

\subsection{Limits of Associativity Reduction}

%As modern systems place no restrictions on the virtual-to-physical mapping, address translation is on the critical path of every memory access. To break translation and data path serialization, prior work proposes to eliminate associativity and enforce a direct mapping between virtual pages and page frames~\cite{picorel:near-memory, haria:devirtualizing}. As every virtual page can only map to one single location, translation and data paths are independent and can proceed in parallel.

As our approach intends to find a middle-ground between fully associative (i.e., conventional translation) and direct mapping~\cite{picorel:near-memory, haria:devirtualizing}, we aim to fill the research void of what is the level of associativity needed. Interestingly enough, there exists no study on VM associativity, unlike caches, which are similar in some organizational aspects and for which such a study has existed for three decades~\cite{hill:aspects}. To study the VM associativity, we employ the well-established 3C model---initially developed for caches~\cite{hill:aspects}. In this context, associativity means the number of possible locations (page frames) a given virtual page can map to. This model classifies misses (i.e., page faults) into three categories: conflict misses (when too many active pages map to a fraction of the sets), capacity misses (due to limited memory size), and compulsory misses (upon the first access to a page). 

%One potential way to break translation-memory access serialization is to completely eliminate associativity and enforce a direct mapping between virtual pages and page frames. Since a virtual page can now only map to a single page frame, address translation and data fetch are independent and can proceed in parallel. Conventional wisdom dictates that direct mapping creates an excess of page faults, due to conflicts from multiple virtual pages mapping to the same page frame. However, this is merely intuition, as there exists no study on VM associativity, unlike caches, which are similar in some organizational aspects and for which such a study has existed for three decades~\cite{hill:aspects}. 


%To fill this research void, we employ the 3C model---initially developed for caches~\cite{hill:aspects}---to study VM associativity. In this context, associativity means the number of possible locations (page frames) a given virtual page can map to. This model classifies misses (i.e., page faults) into three categories: conflict misses (when too many active pages map to a fraction of the sets), capacity misses (due to limited memory size), and compulsory misses (upon the first access to a page). 

%To simulate real-world scenarios, we select a set of representative server workloads, summarized in Table~\ref{table:workload}. We include two cloud workloads from CloudSuite~\cite{ferdman:clearing}, Cassandra and Memcached, an online transaction processing (OLTP) workload~\cite{facebook:linkbench}, MySQL, two online analytical processing (OLAP) workloads~\cite{boncz:breaking}, TPC-H and TPC-DS, and a widely-used storage system workload~\cite{dong:optimizing}, RocksDB. We collect long memory traces of several 10s of billions of instructions of the server workloads using Pin~\cite{luk:pin}. We extract the virtual address and address space identifier (ASID) of each memory reference and use it to probe a set-associative memory structure, varying the associativity to observe and classify the misses. A detailed description of our methodology is found in Section~\ref{sec:methodology}.

%\subsection{Single-Process In-Memory}

%Fig.~\ref{fig:miss_ratio} shows results for three single-process
%workloads: Memcached, RocksDB, and Cassandra. The y-axis breaks down
%the total misses into the three distinct miss classes. Each category
%on the x-axis corresponds to the ratio between the size of the
%physical memory and the size of the application's working set. For
%example, $8\times$ indicates that the memory is eight times larger
%than the application's working set. Similarly, $1/2\times$ means that
%the application's working set is twice the size of the memory. We
%collapse results for $8\times$ to $1\times$ because they are similar
%and visually identical; each case represents a fully in-memory
%scenario. Furthermore, within each working set category, the x-axis
%sweeps through different VM associativities, from direct-mapped to
%32-way associative.


We collect memory traces using Pin~\cite{luk:pin}. For workloads with fine-grained operations (i.e., Memcached, RocksDB, MySQL, and Cassandra), the traces contain the same number of instructions as the application executes in 60 seconds without Pin. For analytics workloads (i.e., TPC-H \& TPC-DS), we instrument the entire execution. We extract the ASID bits and virtual address of each memory access, concatenate both~\cite{basu:reducing, yoon:revisiting},  and use it to probe a set-associative memory structure. For the associativity experiments, we tune all the workloads to have a resident set size (RSS) of 8GB. In other words, the allocated physical memory for all the processes of a given workload is 8GB. For single-process runs, a single process has an RSS of 8GBs. For two-process runs, each of the processes has an RSS of 4GB. The same scaling applies for the runs with four and eight processes. To vary the memory size to working set size ratio, we vary the size of the set-associative memory structure. 

%We use the same traces for the page table experiments of Section~\ref{sec:pagetable} and feed them into our inverted page table modeling tool. For the TLB experiments in Section~\ref{sec:tlb} and the performance experiments in Section~\ref{sec:evaluation}, we tune the workloads to employ working sets of size 32GB and 64GB, depending on the size of the network. We use 32GB and 64GB for 4- and 8-chip, and 16-chip configurations respectively.

We collect the traces on a dual-socket server CPU (Intel Xeon E5-2680 v3) with $256$GB of memory, using the Linux 3.10 kernel and Google's TCMalloc~\cite{google:tcmalloc}. Address space randomization (ASLR) is enabled in all experiments.

\subsection{Performance}


Full-system simulation for TLB misses and page faults is not practical, as these events occur less frequently than other micro-architectural events (e.g., branch mispredictions). Hence, we resort to the CPI models often used in VM research~\cite{papadopoulou:prediction-based, saulsbury:recently-based, bhattacharjee:shared} to sketch the performance gains. These prior studies report performance as the reduction in the translation-related cycles per instruction. As CPI components are additive, this metric is valid irrespective of the workload's baseline CPI. We further strengthen this methodology by studying the CPI savings on all memory operations, not only on translation (as we overlap translation and data fetch operations). Our model thus captures both the translation and data fetch cycles, which together constitute the largest fraction of the total CPI in data structure traversals~\cite{picorel:near-memory}. The CPI is measured by applying fixed penalties to the TLB miss rates obtained with the PIN traces. 

