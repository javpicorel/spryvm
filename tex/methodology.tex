\section{Methodology}
\label{sec:methodology}

Like most recent work on VM~\cite{basu:efficient, pham:increasing, pham:colt, bhattacharjee:large-reach, barr:spectlb, papadopoulou:prediction-based, saulsbury:recently-based}, we use trace-driven functional and cycle-accurate simulation.

\subsection{Traces}

We collect memory traces using Pin~\cite{luk:pin}. For workloads with fine-grained operations (i.e., Memcached, RocksDB, MySQL, and Cassandra), the traces contain the same number of instructions as the application executes in 60 seconds without Pin. For analytics workloads (i.e., TPC-H \& TPC-DS), we instrument the entire execution. We extract the ASID bits and virtual address of each memory access, concatenate both~\cite{basu:reducing, yoon:revisiting},  and use it to probe a set-associative memory structure. 

For the associativity experiments in Section~\ref{sec:associativity}, we tune all the workloads to have a resident set size (RSS) of 8GB. In other words, the allocated physical memory for all the processes of a given workload is 8GB. For single-process runs, a single process has an RSS of 8GBs. For two-process runs, each of the processes has an RSS of 4GB. The same scaling applies for the runs with four and eight processes. To vary the memory size to working set size ratio, we vary the size of the set-associative memory structure. We use the same traces for the page table experiments of Section~\ref{sec:pagetable} and feed them into our inverted page table modeling tool. For the TLB experiments in Section~\ref{sec:tlb} and the performance experiments in Section~\ref{sec:evaluation}, we tune the workloads to employ working sets of size 32GB and 64GB, depending on the size of the network. We use 32GB and 64GB for 4- and 8-chip, and 16-chip configurations respectively.

We collect the traces on a dual-socket server CPU (Intel Xeon E5-2680 v3) with $256$GB of memory, using the Linux 3.10 kernel and Google's TCMalloc~\cite{google:tcmalloc}. Address space randomization (ASLR) is enabled in all experiments.

\subsection{Performance}

%%%% THINK WHETHER TO ADD IT BACK
%This study does not cover the problem of partitioning applications into CPU-side and memory-side execution, or specializing the latter in hardware, or interfacing and synchronizing the two executions~\cite{gao:practical, ahn:scalable, balasubramonian:near-data}. Such problems are out of the scope. To avoid such problems we evaluate the applications running entirely on the MPUs.

Full-system simulation for TLB misses and page faults is not practical, as these events occur less frequently than other micro-architectural events (e.g., branch mispredictions). Hence, we resort to the CPI models often used in VM research~\cite{papadopoulou:prediction-based, saulsbury:recently-based, bhattacharjee:shared} to evaluate the performance gains. These prior studies report performance as the reduction in the translation-related cycles per instruction. As CPI components are additive, this metric is valid irrespective of the workload's baseline CPI. We further strengthen this methodology by studying the CPI savings on all memory operations, not only on translation (as we overlap translation and data fetch operations). Our model thus captures both the translation and data fetch cycles, which together constitute the largest fraction of the total CPI in server workloads~\cite{ferdman:clearing}. The CPI is measured by feeding the memory traces into our cycle-accurate simulator. 

\subsection{Simulation Parameters}
We use the Flexus cycle-accurate simulator~\cite{wenisch:simflex}, with detailed core, MMU, memory hierarchy, and interconnect models. Following prior work on MPUs~\cite{gao:practical, ahn:scalable, pugsley:ndc}, we model the MPU cores after ARM Cortex A7~\cite{arm:cortex-a7}. We provision the baseline with a high-end MMU similar to Intel Xeon Haswell~\cite{intel:tlbs, mammarlund:4th}, with multi-level TLBs and MMU caches~\cite{bhattacharjee:large-reach, barr:translation}. For the baseline, we consider a 4-level hierarchical page table~\cite{jacob:look} (as in ARMv$8$ and x$86\_64$) and an inverted page table identical to the one spryVM employs, but covering the whole memory. We assume 48-bit virtual and physical addresses. The MMU supports $4$KB, $2$MB, and $1$GB pages. Page table entries are transparently allocated in the L$1$-D cache like in commercial systems~\cite{intel:architectures}. For simplicity, we probe the cache with physical addresses for the baseline and with virtual addresses when using spryVM. Both behave identically as we verify that TLB misses never reference a cache-resident block.

We model our 3D memory stack following the organization of the Micron's Hybrid Memory Cube with eight $8$Gb DRAM layers and 16 vaults~\cite{micron:hmc}. We conservatively estimate DRAM timing parameters from publicly available information and research literature~\cite{gao:practical}. For spryVM, we employ a conventional two-level TLB hierarchy. The SRAM overhead per memory chip is less than 256KB (which is partitioned across vaults) for an area of $0.3mm^2$ in $22$nm, corresponding to less than 0.2\% of the area of an $8$Gb DRAM die (i.e., $226mm^2$~\cite{shevgoor:quantifying}). As there are 16 memory partitions (or vaults), each of $512$MB, spryVM provides a VM associativity of $128$K ways, making the number of page faults virtually identical to full associativity.

We conservatively assume that page faults always involve SSD IO, taking $32\mu{}s$ to resolve~\cite{chou:cameo}. This parameter is only important for out-of-memory workloads. Memory-resident workloads only suffer a page fault the first time a page is accessed. Table~\ref{table:system} summarizes all system parameters.

%%%%%%%%%%% Table: Methodology %%%%%%%%%%%
\begin{table}
	\begin{center}
		\caption{System parameters.}
		\scalebox{0.7}
		\small
		\vspace{0.01in}
		\label{table:system}
		\renewcommand{\arraystretch}{1.0} 
		{\scriptsize
			\begin{tabular}{ l  l }
				%\hline
				\toprule
				{\bf MPU logic}                  & {\bf Description}  \\
				%\hline
				%\hline
				\toprule
				\multirow{1}{*}{Cores}                       &  Single-issue, in-order, 2GHz \\ 
				\cmidrule{2-2}
				\multirow{1}{*}{L1-I/D}                       &  32KB, 2-way, 64B block, 2-cycle load-to-use \\ 
				\toprule
				{\bf MMU}                  & {\bf Description}  \\
				\toprule
				
				%\hline
				
				%\multirow{2}{*}{Core Types}    & In-order (Cortex A8-like): 2-wide \\
				%		                               &  OoO (Xeon-like): 4-wide, 128-entry ROB \\
				%\hline
				\multirow{3}{*}{TLB}	& $4$KB pages: 64-entry, 4-way associative \\
				& $2$MB pages: 32-entry, 4-way associative \\
				& $1$GB pages: 4-entry, fully associative \\
				\cmidrule{2-2}
				%\multirow{2}{*}{L1-I/D Caches}	&	32KB, split, 2 ports, 64B blocks, 10 MSHRs, \\
				%							& 2-cycle load-to-use latency \\
				\multirow{1}{*}{STLB} 			& 4KB/2MB pages: 1024-entry, 8-way associative \\ 
				
				%\hline
				\cmidrule{2-2}
				\multirow{3}{*}{Caches} 	&	L4: 2-entry, fully associative~\cite{bhattacharjee:large-reach} \\ 
				&       L3: 4-entry, fully associative~\cite{bhattacharjee:large-reach} \\
				&       L2: 32-entry, 4-way associative~\cite{bhattacharjee:large-reach} \\
				%			\cmidrule{2-2}
				%			\multirow{1}{*}{Walker} 	&	Two in-flight page walks \\ 
				
				\toprule
				{\bf Memory}                  & {\bf Description}  \\
				\toprule
				
				
				\multirow{1}{*}{MPU chip}                                   &  8GB chips, 8 DRAM layers x 16 vaults \\         
				\cmidrule{2-2}
				\multirow{1}{*}{Networks}                             &  4, 8, and 16 chips in daisy chain and mesh \\       
				\cmidrule{2-2}
				\multirow{2}{*}{DRAM}                                &  $t_{CK} = 1.6$ns, $t_{RAS} = 22.4$ns, $t_{RCD} = 11.2$ns \\
				&  $t_{CAS} = 11.2$ns, $t_{WR} = 14.4$ns, $t_{RP} = 11.2$ns \\ 
				\cmidrule{2-2}
				\multirow{1}{*}{Serial links}                             &  2B bidirectional, 10GHz, 30ns per hop~\cite{kanter:cavium, towles:unifying} \\           
				\cmidrule{2-2}
				\multirow{1}{*}{NoC}                             &  Mesh, 128-bit links, 3 cycles per hop \\
				%\hline
				
				\toprule
				{\bf spryVM}                  & {\bf Description}  \\
				\toprule
				\multirow{1}{*}{TLB}	& $4$KB pages: 64-entry, 4-way associative \\
				\cmidrule{2-2}
				%\multirow{2}{*}{L1-I/D Caches}	&	32KB, split, 2 ports, 64B blocks, 10 MSHRs, \\
				%							& 2-cycle load-to-use latency \\
				\multirow{1}{*}{STLB} 			& 4KB pages: 1024-entry, 8-way associative \\ 
				
				
				\bottomrule
				%\hline
			\end{tabular}
		} %small
	\end{center}
	\vspace{-0.1in}
\end{table}