\section{Operating System Support}
\label{sec:os}

One of SpryVM's advantages is that is is readily-implementable and
compatible with commodity OSes. To demonstrate this, we have
prototyped SpryVM in stock Linux by implementing set-associativity in
the memory management sections of the kernel. Additionally, we have
implemented SpryVM's page table in the kernel. (We focus on an
inverted page table implementation, though other implementations are
also just as readily possible.) Overall, these and other ancillary
modifications were compatible with the existing kernel. A measure of
this is that when use \verb|git diff| to track our changes, we find
the following: \texttt{20 files changed, 266 insertions(+), 43
  deletions(-)}. Key aspects of our changes involved:

\vspace{2mm}
\noindent {\bf Set-associativity using NUMA page allocation.} We were
able to greatly simplify our set-associativity implementation by
leveraging existing Linux code built to bind processes to NUMA nodes
(i.e., {\it membind}). In other words, we observed that the notion of
mapping only a fixed set of physical frames to a virtual page is
analogous to the idea of assigning physical frames from only select
NUMA nodes. Consequently, to build set-associativity, we conceptually
treat subsets of the virtual address space as being bound to specific
NUMA nodes. Therefore SpryVM can be implemented in a manner that
reuses {\it membind} code paths such that each memory set is treated
as a NUMA node. 

\vspace{2mm}
\noindent {\bf Anonymous and file-backed pages.} Our SpryVM
implementation has to treat anonymous and file-backed pages
differently because the former is generally private to a process,
while the latter can (and is) often shared among processes.

Consider the case of anonymous pages. On page faults to them, the OS
must calculate a corresponding NUMA node ID when allocating physical
pages. To do this, we embed logic in the kernel to apply our hash
function on the faulting virtual page before obtaining a free page
from the NUMA node identified by the hash function. We found it most
efficient to embed this logic in Linux's \verb|alloc_pages_vma()|,
which serves as the entry point of all user space allocations. Within
this function, we calculate a memory set index (stored in the NUMA
node mask), derive the desired zone list, and pass this information to
the page allocation function, (\verb|__alloc_pages_nodemask()|). From
this point onwards, we recycle existing Linux code paths for anonymous
pages. The basic operations are shown in
Algorithm~\ref{alg:anonymous}.

File-backed pages, on the other hand, present more subtle
implementation challenges because they can be shared among different
processes. Because of this, there is no guarantee that multiple
processes use the same virtual addresses to access these pages,
meaning that the output of our hash function may differ depending on
the faulting process. Consequently, we use the page offset in a file
as the memory set ID for each page. The rationale is that page offsets
indicate the position of the page residing in the file to which it
maps. Therefore, using page offsets enables us to allocate file-backed
pages from different memory sets via our index scheme. We do need to
adjust virtual address assignment in the kernel (via
\verb|get_unmapped_area()|) to accomplish this however. We do this by
aligning the beginning of the given virtual address to match memory
set 0's index. Thus, each process maps files starting from memory set
0. These algorithms are shown in Algorithm~\ref{alg:file-backed}.


\vspace{2mm}
\noindent {\bf Inverted page table design.} We adopt the same inverted
page design as the hashed page tables in IBM Power systems. We reserve
memory for each inverted page table in each individual NUMA node and
insert/invalidate/update inverted page table entries whenever CPU page
tables change in the same hook functions
(e.g. \verb|update_mmu_cache()|). To guarantee coherence in
translation information between CPUs and accelerators, each inverted
page table entry has a valid bit atomically accessed by both CPUs (for
inverted page table modification) and accelerators (for
reading). 

  
\begin{comment}
Although end-to-end performance evaluation requires a full implementation of spryVM in a modern OS (e.g., Linux, FreeBSD), like prior work which require OS changes \cite{talluri:surpassing, talluri:tradeoffs, talluri:pagetable}, we note that such implementation is a multi-man-year project. Consequently, like those studies, we instead inspect an open-source OS (i.e., Linux) and qualitatively discuss our proposed modifications.  

Fortunately, our two main modifications (i.e., memory-set matching and managing multiple page tables) are currently implemented in modern OS's. For instance, FreeBSD implements page coloring to avoid consecutive virtual pages mapping to the same cache set. FreeBSB does this with a free list of pages per cache set~\cite{mckusick:design}, guaranteeing that two consecutive virtual pages map to different cache sets. We can similarly extend the page placement policy to make both the VPN and PFN map to the same memory set. Further, the Linux Heterogeneous Memory Management (HMM) patch sets up multiple page tables and maintains the consistency~\cite{glisse:hmm}. We can leverage the lessons learned from these patches.

Table~\ref{tab:table_modifications} and Table~\ref{tab:table_overheads} shed more light on the OS changes and the potential overheads respectively. Although set associative VM is beneficial for a wide range of workloads, there might be cases for which full associative VM is required. Hence, we envision spryVM to be an optional optimization (i.e., a process, set of processes, or the whole system can fall back to the conventional translation). This is conceptually similar to the way OSes support superpages, for which large portions of the OS required modifications along with their associated overheads with respect to supporting a single page size~\cite{talluri:surpassing, navarro:practical, kwon:coordinated}. Analogously, when super pages are not beneficial or impossible to generate, processes fall back to base pages. 
 
 %MIPS OS searched the free list of pages to find a page frame that maps to the same first-level cache set as the virtual page (i.e., page coloring)~\cite{taylor:tlb}.

%We could basically say that what we're doing uses known techniques to leverage a novel observation (set-associativity). 

%As a specific example, take the fact that we're setting up multiple page tables. Well, the latest Linux HMM patch does exactly this, so Linux developers already tackled all the challenges of syncing up two page tables. We could basically say that what we're doing uses known techniques to leverage a novel observation (set-associativity). 

%RMM requires modest operating system (OS) modifications.The OS must create and manage range table entries in softwareand coordinate them with the page table. We modify the OS to increase the size of ranges with an eager paging allocation mechanism. We prototype these changes in Linux, but thedesign is applicable to other OSes.

%The really big concern that we have to hit out of the park is the last MICRO reviewer's concern of OS changes. I agree that fully implementing this in Linux/FreeBSD/sv6/etc. would instill more confidence that spryVM/spryVM works effectively. However, I also don't think that this is a reasonable bar for an academic paper. To address this, my first instinct is to go the following route:
\end{comment}

\begin{comment}
\subsection{Support spryVM with NUMA page allocation policy}
In Linux,  mapping only a subset of physical pages to given virtual addresses is very similar to a scenario existing in NUMA systems — binding the memory of processes to a specific NUMA nodes. With NUMA memory bind policy (i.e. \textit{membind}), a process is only able to get physical pages coming from the NUMA node assigned by the policy, where, with spryVM, each subset of a process’s virtual address space can be seen as being bound to a certain NUMA node. As a result, to implement spryVM in operating systems, we are able to reuse most of existing NUMA-related code. In a spryVM system, each memory set can be treated as a NUMA node and all processes have \verb|membind| as their default memory allocation policy.

In Linux, there are two types of pages, anonymous pages and file-backed pages. The former
is generally private to each process and the latter can be shared between different processes.
Thus, they need to handled differently.

\textbf{Anonymous pages} Whenever a page fault happens, during the process of physical page allocation, the operating system calculates the corresponding NUMA node id from the faulting virtual address with our hash function, then obtains a free page from that NUMA node and finishes the page fault process. Linux uses \verb|alloc_pages_vma()| as the entry point of each user space  page allocation, thus, we simply calculate a memory set index (stored in a NUMA node mask variable) out of the faulting address, derive the corresponding zone list, and pass them to the page allocation function (\verb|__alloc_pages_nodemask()|). Linux is able to handle the rest of page fault work without any problem. The basic operations are shown in Algorithm~\ref{alg:anonymous}.

\textbf{File-backed pages} File-backed pages can be shared among different processes but
allocating file-backed pages based on faulting virtual addresses only works when they belong to
only one process. Because there is no guarantee that other processes will map a file-backed page
with the same virtual address or an virtual address having the same memory index as the first
virtual address mapping to the page. To solve this, we use page offset in a file as the memory set
id for each page. A page offset tells the position of the page residing in the file to which it maps.
For example, for a 64KB file, there will be 8 pages mapping to it and first page gets page offset
0, second gets page offset 1, and so on. Thus, all file-backed pages are allocated from different
memory sets based on our index scheme. On the other hand, we need to adjust virtual address 
assignment in the kernel, which comes from \verb|get_unmapped_area()|. All we need to do
is aligning the beginning of the given virtual address to match memory set 0's index. Thus,
each process maps a file all starting from memory set 0. Both algorithms are shown in 
Algorithm~\ref{alg:file-backed}.


\textbf{Inverted page table} Not all architectures use inverted page tables, but they are commonly used in IBM Power systems as hashed page tables. Thus, we adopt the same operation model as hashed page tables in IBM Power systems: reserving memory for each inverted page table in individual NUMA node and inserting/invalidating/updating inverted page table entries whenever CPU page tables change in the same hook functions (e.g. \verb|update_mmu_cache()|). To guarantee the translation information is coherent between CPUs and accelerators, each inverted page table entry has a valid bit, which is atomically accessed by both CPUs (for inverted page table modification) and accelerators (for reading). Alternatively, if accelerators, e.g. GPUs, want to maintain their own page tables instead of using the inverted page table, additional hook functions, like  \verb|mmu_notifier| mechanism in Linux, could be used to keep the page tables of accelerators coherent with CPU page tables.

All these modifications are very simple and not intrusive to operating systems. \verb|git diff| shows the summary of our changes, \texttt{20 files changed, 266 insertions(+), 43 deletions(-)}, in Linux to achieve the required functionality for spryVM. 
\end{comment}

\begin{algorithm}
  \caption{Anonymous page allocation scheme}\label{alg:anonymous}
  \begin{algorithmic}[1]
    \Procedure{alloc\_pages\_vma}{\textbf{addr\_t} vaddr}
    \State $\textit{mem\_set\_id} \gets \verb|MEM_SET_INDEX_HASH|(\mbox{vaddr})$
    \State $\textit{page} \gets \verb|alloc_pages_node|(\mbox{vaddr}, \mbox{mem\_set\_id})$
    \State \textbf{return} \textit{page}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{File-backed page allocation scheme}\label{alg:file-backed}
  \begin{algorithmic}[1]
    \Procedure{alloc\_page\_cache}{\textbf{pgoff\_t} page\_offset}
    \State $\textit{mem\_set\_id} \gets \mbox{page\_offset}$
    \State $\textit{page} \gets \verb|alloc_pages_node|(\mbox{vaddr}, \mbox{mem\_set\_id})$
    \State \textbf{return} \textit{page}
    \EndProcedure

    \Procedure{get\_unmapped\_area}{\textbf{size\_t} mmap\_size, \textbf{pgoff\_t} page\_offset}
    \State $\textit{vaddr} \gets \verb|FIND_HOLE_IN_VSPACE|(\mbox{mmap\_size})$
    \State $\textit{vaddr} \gets \verb|ALIGN_TO_OFFSET|(\mbox{vaddr}, \mbox{page\_offset})$
    \State \textbf{return} \textit{vaddr}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
