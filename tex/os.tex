\section{DTRIM: Operating System Support}
\label{sec:os}

One of DTRIM's advantages is that is is readily-implement\-able and
compatible with commodity OSes. To demonstrate this, we have
prototyped DTRIM in stock Linux (4.10) by implementing
set-associativity in the memory management sections of the
kernel. Additionally, we have implemented DTRIM's page table in the
kernel as an inverted page table, although other implementations are
also just as readily possible.  Overall, these and other ancillary
modifications were compatible with the existing kernel. A measure of
this is that when use \verb|git diff| to track our changes, we find
the following: \texttt{20 files changed, 266 insertions(+), 43
  deletions(-)}. Key aspects of our changes involved:

\vspace{2mm}
\noindent {\bf Set-associativity using NUMA page allocation.} We were
able to simplify our set-associativity implementation by leveraging
existing Linux code built to bind processes to NUMA nodes (i.e., {\it
  membind}). In other words, we observed that the notion of mapping
only a fixed set of physical frames to a virtual page is analogous to
the idea of assigning physical frames from only select NUMA
nodes. Consequently, to build a system with only slightly reduced
associativity, we conceptually treat subsets of the virtual address
space as being bound to specific NUMA nodes. Therefore DTRIM can be
implemented in a manner that reuses {\it membind} code paths. Note
that this implementation is only possible because DTRIM reduces
associativity only \textit{by a small factor}, as opposed to reducing
it \textit{to a small number} (e.g., direct mapped or 4-way
associative~\cite{picorel:near-memory}). We will show, however, that
only a small reduction in associativity is practically needed for
significant performance improvements.


\vspace{2mm}
\noindent {\bf Virtual page and physical page allocation.} When
implementing DTRIM in Linux, we needed to ensure that each virtual
page mapped to a physical page in the right memory set based on our
{\it MEM\_SET\_INDEX\_HASH()} function.  Two situations are possible:
1) a virtual page maps to a physical page that is not present in the
memory, which mostly happens when this physical page is
\textit{private} and a subsequent page fault will allocate a physical
page to this virtual page; 2) a virtual page maps to a physical page
that is already present in the memory, which means either this
physical page is \textit{shared} between processes via the virtual
page or the virtual page is a \textit{remap} of this physical page.
Our solution is to adjust virtual pages or physical pages according to
the existing memory information. If no physical page is present in the
memory, we allocate physical pages according to the memory set index
based on virtual addresses. If a physical page is present, we assign a
virtual address that maps to the memory set, which the existing
physical page resides in, based on our memory set index
scheme. Concrete examples are discussed below:

\begin{itemize}
  \item \textbf{Private physical pages.} In this case, because no
    physical page is in the memory before any page fault, we place no
    restriction on virtual address assignment.  This means we use
    Linuxâ€™s existing \verb|get_unmap_area|() kernel function to get a
    free virtual address region and assign it to a user space
    application whenever the user space application request virtual
    address space regions via malloc or such.  When page faults
    happen, since there is no corresponding physical page in the
    memory, we first calculate the memory set index from the faulting
    virtual address and allocate a physical page, using Linux's NUMA
    page allocation function, from the corresponding memory set. This
    is shown in Algorithm~\ref{alg:private}.
  
  \item \textbf{Shared physical pages.} In this case, because a
    physical page is already present in a specific memory set, we have
    to restrict our virtual address assignment.  This means after we
    get a free virtual address region upon a request from user space
    applications by using the kernel function \verb|get_unmap_area|(),
    we need to adjust the starting and ending addresses of the virtual
    address region, so that all the virtual pages in the region map to
    the existing physical pages and conform to our {\it
      MEM\_SET\_INDEX\_HASH()} function. For example, in a system with
    4 memory sets, whose physical page regions are [P0...P9],
    [P10...P19], [P20...P29], [P30...P39], and our memory set index
    hash function is VPN mod 4.  At the beginning, a process Proc1
    maps its virtual memory region [$V3_{Proc1}$ ... $V7_{Proc1}$] to
    a group of physical pages [P30, P0, P10, P20, P31] from memory
    sets 3, 0, 1, 2, 3, respectively.  Next a second process Proc2 is
    calling \verb|mmap|() to share the same set of physical pages with
    P1. At that moment, the \verb|get_unmap_area|() kernel function
    can give [$V5_{Proc2}$ ... $V9_{Proc2}$] to P2 as its virtual
    memory region pointing to the shared physical pages. But this
    virtual memory region maps to a sequence of physical pages from
    memory sets 1, 2, 3, 0, 1, which does not match the memory set
    sequence of the existing physical pages.  In that case, inside the
    kernel, we need to adjust the region from
    [$V5_{Proc2}...V9_{Proc2}$] to [$V7_{Proc2}...V11_{Proc2}$] or
    other analogous regions, so that the adjusted virtual memory
    region maps to a sequence of physical pages from memory sets 3, 0,
    1, 2, 3, which is the same sequence as the shared physical pages.

  \item \textbf{Remapped physical pages.} This case is almost the same as the shared memory case, 
  since it basically assigns a new virtual address region to an existing set of physical pages. 
  Thus, we will first find a suitable virtual memory region, then adjust the virtual address region
  that it maps to the same memory set sequence of the group of existing physical pages and return
  it to the userspace application, which asks for memory remapping via \verb|mremap|() system call.

\end{itemize}


\vspace{2mm}
\noindent {\bf Inverted page table design.} We adopt the same inverted
page design as the hashed page tables in IBM Power systems. We reserve
memory for each inverted page table in each individual NUMA node and
insert/invalidate/update inverted page table entries whenever CPU page
tables change in the same hook functions
(e.g. \verb|update_mmu_cache()|). To guarantee coherence in
translation information between CPUs and accelerators, each inverted
page table entry has a valid bit atomically accessed by both CPUs (for
inverted page table modification) and accelerators (for
reading). 

\vspace{2mm}{\noindent \bf Summary:} Overall, all these changes can be
subsumed in the kernel's code paths without requiring any changes to
the memory management API exposed to programmers. Moreover, every
single memory management function provided by stock Linux (e.g., {\it
  malloc}, {\it mmap}, {\it remap}, etc.) maintains the same
functionality to the end-user. Therefore, set-associativity is
achieved completely transparently to application developers and/or
system administrators. Moreover, DTRIM is orthogonal to superpages and
can be used with or without them.

  
\begin{comment}
Although end-to-end performance evaluation requires a full implementation of DTRIM in a modern OS (e.g., Linux, FreeBSD), like prior work which require OS changes \cite{talluri:surpassing, talluri:tradeoffs, talluri:pagetable}, we note that such implementation is a multi-man-year project. Consequently, like those studies, we instead inspect an open-source OS (i.e., Linux) and qualitatively discuss our proposed modifications.  

Fortunately, our two main modifications (i.e., memory-set matching and managing multiple page tables) are currently implemented in modern OS's. For instance, FreeBSD implements page coloring to avoid consecutive virtual pages mapping to the same cache set. FreeBSB does this with a free list of pages per cache set~\cite{mckusick:design}, guaranteeing that two consecutive virtual pages map to different cache sets. We can similarly extend the page placement policy to make both the VPN and PFN map to the same memory set. Further, the Linux Heterogeneous Memory Management (HMM) patch sets up multiple page tables and maintains the consistency~\cite{glisse:hmm}. We can leverage the lessons learned from these patches.

Table~\ref{tab:table_modifications} and Table~\ref{tab:table_overheads} shed more light on the OS changes and the potential overheads respectively. Although set associative VM is beneficial for a wide range of workloads, there might be cases for which full associative VM is required. Hence, we envision DTRIM to be an optional optimization (i.e., a process, set of processes, or the whole system can fall back to the conventional translation). This is conceptually similar to the way OSes support superpages, for which large portions of the OS required modifications along with their associated overheads with respect to supporting a single page size~\cite{talluri:surpassing, navarro:practical, kwon:coordinated}. Analogously, when super pages are not beneficial or impossible to generate, processes fall back to base pages. 
 
 %MIPS OS searched the free list of pages to find a page frame that maps to the same first-level cache set as the virtual page (i.e., page coloring)~\cite{taylor:tlb}.

%We could basically say that what we're doing uses known techniques to leverage a novel observation (set-associativity). 

%As a specific example, take the fact that we're setting up multiple page tables. Well, the latest Linux HMM patch does exactly this, so Linux developers already tackled all the challenges of syncing up two page tables. We could basically say that what we're doing uses known techniques to leverage a novel observation (set-associativity). 

%RMM requires modest operating system (OS) modifications.The OS must create and manage range table entries in softwareand coordinate them with the page table. We modify the OS to increase the size of ranges with an eager paging allocation mechanism. We prototype these changes in Linux, but thedesign is applicable to other OSes.

%The really big concern that we have to hit out of the park is the last MICRO reviewer's concern of OS changes. I agree that fully implementing this in Linux/FreeBSD/sv6/etc. would instill more confidence that DTRIM/DTRIM works effectively. However, I also don't think that this is a reasonable bar for an academic paper. To address this, my first instinct is to go the following route:
\end{comment}

\begin{comment}
\subsection{Support DTRIM with NUMA page allocation policy}
In Linux,  mapping only a subset of physical pages to given virtual addresses is very similar to a scenario existing in NUMA systems â€” binding the memory of processes to a specific NUMA nodes. With NUMA memory bind policy (i.e. \textit{membind}), a process is only able to get physical pages coming from the NUMA node assigned by the policy, where, with DTRIM, each subset of a processâ€™s virtual address space can be seen as being bound to a certain NUMA node. As a result, to implement DTRIM in operating systems, we are able to reuse most of existing NUMA-related code. In a DTRIM system, each memory set can be treated as a NUMA node and all processes have \verb|membind| as their default memory allocation policy.

In Linux, there are two types of pages, anonymous pages and file-backed pages. The former
is generally private to each process and the latter can be shared between different processes.
Thus, they need to handled differently.

\textbf{Anonymous pages} Whenever a page fault happens, during the process of physical page allocation, the operating system calculates the corresponding NUMA node id from the faulting virtual address with our hash function, then obtains a free page from that NUMA node and finishes the page fault process. Linux uses \verb|alloc_pages_vma()| as the entry point of each user space  page allocation, thus, we simply calculate a memory set index (stored in a NUMA node mask variable) out of the faulting address, derive the corresponding zone list, and pass them to the page allocation function (\verb|__alloc_pages_nodemask()|). Linux is able to handle the rest of page fault work without any problem. The basic operations are shown in Algorithm~\ref{alg:anonymous}.

\textbf{File-backed pages} File-backed pages can be shared among different processes but
allocating file-backed pages based on faulting virtual addresses only works when they belong to
only one process. Because there is no guarantee that other processes will map a file-backed page
with the same virtual address or an virtual address having the same memory index as the first
virtual address mapping to the page. To solve this, we use page offset in a file as the memory set
id for each page. A page offset tells the position of the page residing in the file to which it maps.
For example, for a 64KB file, there will be 8 pages mapping to it and first page gets page offset
0, second gets page offset 1, and so on. Thus, all file-backed pages are allocated from different
memory sets based on our index scheme. On the other hand, we need to adjust virtual address 
assignment in the kernel, which comes from \verb|get_unmapped_area()|. All we need to do
is aligning the beginning of the given virtual address to match memory set 0's index. Thus,
each process maps a file all starting from memory set 0. Both algorithms are shown in 
Algorithm~\ref{alg:file-backed}.


\textbf{Inverted page table} Not all architectures use inverted page tables, but they are commonly used in IBM Power systems as hashed page tables. Thus, we adopt the same operation model as hashed page tables in IBM Power systems: reserving memory for each inverted page table in individual NUMA node and inserting/invalidating/updating inverted page table entries whenever CPU page tables change in the same hook functions (e.g. \verb|update_mmu_cache()|). To guarantee the translation information is coherent between CPUs and accelerators, each inverted page table entry has a valid bit, which is atomically accessed by both CPUs (for inverted page table modification) and accelerators (for reading). Alternatively, if accelerators, e.g. GPUs, want to maintain their own page tables instead of using the inverted page table, additional hook functions, like  \verb|mmu_notifier| mechanism in Linux, could be used to keep the page tables of accelerators coherent with CPU page tables.

All these modifications are very simple and not intrusive to operating systems. \verb|git diff| shows the summary of our changes, \texttt{20 files changed, 266 insertions(+), 43 deletions(-)}, in Linux to achieve the required functionality for DTRIM. 
\end{comment}

\begin{algorithm}
  \caption{Physical page allocation scheme.}\label{alg:private}
  \begin{algorithmic}[1]
    \Procedure{alloc\_pages\_vma}{\textbf{addr\_t} vaddr}
    \State $\textit{mem\_set\_id} \gets \verb|MEM_SET_INDEX_HASH|(\mbox{vaddr})$
    \State $\textit{page} \gets \verb|alloc_pages_node|(\mbox{vaddr}, \mbox{mem\_set\_id})$
    \State \textbf{return} \textit{page}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{comment}
\begin{algorithm}
  \caption{Virtual page assignment scheme}\label{alg:shared_or_remap}
  \begin{algorithmic}[1]
    \Procedure{get\_unmapped\_area}{\textbf{size\_t} mmap\_size, \textbf{int} mem\_set\_idx}
    \State $\textit{vaddr} \gets \verb|FIND_HOLE_IN_VSPACE|(\mbox{mmap\_size})$
    \State $\textit{vaddr} \gets \verb|ALIGN_TO_OFFSET|(\mbox{vaddr}, \mbox{mem\_set\_idx})$
    \State \textbf{return} \textit{vaddr}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
\end{comment}
