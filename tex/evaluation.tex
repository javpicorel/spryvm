\section{Evaluation}
\label{sec:evaluation}

\javier{Points to come across:

\begin{itemize}
  \item Set-associativity doesn't increase page faults: 3C model + Memcached and RocksDB on real HW
  \item TLB miss rate vs. working set for microkernels (hash table, skip list, bst internal, bst external)
  \item TLB miss penalty vs. working set (or sockets)
  \item Fragmentation
  \item $IPC =IPC_base + Penalty_tlb + Penalty_page_faults$
\end{itemize}

}


\begin{figure*}[!h]
\centering
 
  \subfloat[In-memory scenario.]{
  \label{fig:speedup_memory_in}
%  \includegraphics[width=.32\textwidth,clip,trim = 18mm 213mm 117mm 20mm]{figs/eps/sim-rread-lat.eps}}
   \includegraphics[width=0.48\textwidth,clip]{graphs/speedup_inmemory.pdf}
   }
%  \hspace{.01in}
%\caption{Speedup over conventional MMU with 4KB pages for 2MB and 1GB pages, SAVAgE, and ideal translation for in memory scenario.}
%\end{figure*}
%\begin{figure*}[!h]
%\centering
  \subfloat[Out-of-memory scenario.]{
  \label{fig:speedup_memory_out}
%  \includegraphics[width=.32\textwidth,clip,trim = 18mm 213mm 115mm 20mm]{figs/eps/sim-rread-bw.eps}}
 \includegraphics[width=0.48\textwidth,clip]{graphs/speedup_disk.pdf}
  }
%  \hspace{.01in}


\caption{Speedup over 4KB-H for 4KB-I, 2MB-H, 2MB-I, spryVM, and an ideal translation.}
  \label{fig:speedup_memory}
 
\end{figure*}

%In this section, we perform a quantitative and qualitative study of the performance of different translation mechanisms.

We now perform a quantitative and qualitative study on VM.

\subsection{Performance Analysis}

The paper discusses many different workload scenarios and system configurations. As there are way too many performance points, we will show only the scenarios that have significant performance differences. First, we will show two workload scenarios, in-memory, where the memory size is equal to the working set size (i.e., the 1x ratio), and out-of-memory, where the working set size is eight times larger than the memory size (i.e., the 1/8x ratio). Second, as the performance across different process counts does not vary significantly, we are showing the average across runs. Third, the two topologies, daisy chain and mesh, behave similarly in terms performance, and hence we also present the average of both. Last, we present the results for different memory chip counts.

Fig.~\ref{fig:speedup_memory_in} shows the speedup of five translation mechanisms over the conventional MMU using 4KB pages and the hierarchical page table, labeled 4KB-H, for the in-memory scenario. The techniques are the conventional MMU using 4KB pages and the inverted page table, 2MB pages using the hierarchical and inverted page tables, spryVM, and an ideal translation mechanism with zero translation overhead. We label the techniques as 4KB-I, 2MB-H, 2MB-I, spryVM, and Ideal respectively. In the interest of space, we omit the results with 1GB pages, which perform better than 4KB pages but always worse than 2MB pages. The reason is that the number of entries in the MMU for 1GB pages is significantly limited, and server workloads have a large number of virtual segments which are accessed concurrently. First, we see that spryVM clearly outperforms 4KB-H, 4KB-I, 2MB-H, and 2MB-I. In some cases by a large margin, up to $25\%$ for TPC-H. In other cases more moderately, $2.1\%$, $2\%$, $1.3\%$, and $0.5\%$ over 4KB-H, 4KB-I, 2MB-H, and 2MB-I respectively for MySQL. Most importantly, spryVM is consistently on par with the ideal translation that incurs zero overhead for translation. Overall, spryVM improves performance by $17.2\%$. $14\%$, $12.3\%$, and $10.5\%$ over 4KB-H, 4KB-I, 2MB-H, and 2MB-I respectively, and stays within 1.2\% of the ideal translation mechanism on average.

Fig.~\ref{fig:speedup_memory_out} presents the speedups for the out-of-memory scenario. As expected, the speedups are less significant that in the in-memory case. The reason is that all the cases incur the slowdown of resolving page faults, and therefore there are fewer accesses that can be accelerated. Still, spryVM systematically performs better than the conventional MMU, $6.1\%$, $5.4\%$, $4.5\%$, and $4\%$ on average over 4KB-H, 4KB-I, 2MB-H, and 2MB-I respectively, and stays within 0.6\% of Ideal. 

Importantly, we model a greatly optimistic case when using $2$MB and $1$GB pages as we assume all pages are huge, with no generation overhead or fragmentation in any scenario. Additionally, we do not account for the excess in IO traffic generated by writing back dirty huge pages. Therefore, we expect further improvements with a realistic overhead. 

%Furthermore, we are conservatively assuming that all conflict page faults are major, and hence involve an access to secondary storage. Hence, in a real system, our speedups would likely be higher.


\subsection{Comparison with Other Proposals}

All the recent proposals on address translation for CPUs presented in Section~\ref{sec:uvm} aim to enhance the reach of TLBs by exploiting the contiguity available in the virtual and physical address spaces. Hence, these techniques are orthogonal to spryVM as we reduce the TLB miss penalty. 

However, these techniques would be only effective for in-memory scenarios, and perform poorly for larger-than-memory working sets. In out-of-memory scenarios, direct segments~\cite{basu:efficient} would not be able to allocate the most frequently used pages in memory. Similarly, Redundant Memory Mappings and Hybrid TLB Coalescing~\cite{karakostas:redundant, park:hybrid} would not be able to allocate contiguous chunks of physical memory and fall back to base pages. CoLT~\cite{pham:colt} and Clustered TLBs ~\cite{pham:increasing} would also perform similar to the conventional translation when there is limited physical contiguity available. 
