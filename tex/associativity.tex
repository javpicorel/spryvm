


%\section{Associativity in Virtual Memory}
\section{SpryVM: Hardware Support}
\label{sec:associativity}

%\javier{ Points to come across:
%
%\begin{itemize}
%  \item We can have a first subsection to talk about the 4 points of the intro's table (i.e., programmability, power/perf, flexibility, safety). What we would like to have, the goal. 
%  \item We can have second subsection to talk about the problems of full associativity, and the idea of set associativity, which is essentially partitioning the memory and translation information and knowing to which memory set to go, and how that can help at the aforesaid 4 points. We can have a high-level figure on how the memory and the translation information is partitioned. 
%  NOTE: This section has to be short, maximum one page, and zero results! It has to be high level!!!
%\end{itemize}

%}

We compare the hardware necessary for SpryVM with conventional
TLBs. We first walk the reader through a translation operation in a
conventional memory management mechanism and then we present the same
operation in SpryVM.

\subsection{Baseline Address Translation}

Figure~\ref{fig:timeline}a and Figure~\ref{fig:timeline}b illustrate the timeline of
conventional address translation. For simplicity, consider the case 
where the accelerator without caches resides close to memory, as
assumed in recent work \cite{haria:devirtualizing, picorel:near-memory}. In the conventional case, a TLB is
first probed by the accelerator it resides in. In case of a TLB hit (Figure~\ref{fig:timeline}a), the translation finishes quickly and 
data fetch starts. Because data can be anywhere in the memory system, in the average case it requires NoC and off-chip network 
traversals to reach the target memory controller. Then the DRAM access is performed and data is routed back through the networks.

In case of a TLB miss (Figure~\ref{fig:timeline}b),  the accelerator's hardware page table walker
identifies the physical address of the page table and initiates a
lookup of it. Page tables can be implemented in many ways, e.g., radix
tree, inverted, hashed, etc. Consider the best-case scenario where either
because of the page table structure or due to the presence of perfect MMU caches, the entire page
table walk amounts to a single memory access\cite{bhattacharjee:large-reach, barr:translation}. This access, however, can 
go to any memory location. Consequently, we may
need to traverse the NoC and off-chip interconnect to reach the
memory channel and memory chip hosting the desired page table entry (PTE). Once the memory
channel is reached, its memory controller reads the PTE from DRAM and sends it to the accelerator, again prompting NoC and
off-chip interconnect traversal. Once the PTE reaches the accelerator, TLB fill occurs. This part of the timeline
constitutes the translation path. Upon successful translation, the accelerator issues the data fetch. A set of NoC and off-chip
interconnects are traversed to reach the memory chip that holds
the requested data. Once the data is read from DRAM, the response has to traverse the same
NoC and off-chip interconnects to reach the accelerator. Th described part of the timeline constitutes the data fetch.

As Figure~\ref{fig:timeline}b shows, conventional address
translation is expensive. First, the TLB's accelerator-side
integration means that it must address all of physical memory, which
elevates its miss rate. Second, when misses occur,
translation and data paths cannot not overlap, and as a result, the entire page walk
latency is on the critical path. Note that accelerators are often not be equipped with hardware page-table walkers, but instead would
ask the IOMMU to perform the walk. In such a case, the miss penalty would be increased even further by the time it takes to communicate with the IOMMU.

\subsection{SpryVM Address Translation}
SpryVM's set-associative organization means that a virtual page can
map to a physical frame from specific memory regions only. This
permits us to relieve the accelerator from having to translate virtual
pages to physical frames and instead, requires that accelerators only
calculate the physical memory region that a virtual page resides in. 
This can be accomplished using a simple hash function (e.g., as simple as using
a subset of the address bits) and the request can be routed to the target region. This means that rather than
placing per-accelerator TLBs, we only need per-region TLBs mapping the
physical frames within that region. Similarly, we can also place a
page table within the memory region for physical pages only belonging
to that region. Per-region TLBs can be made much smaller than
per-accelerator TLBs, which have to cover all of physical
memory. Per-region page tables reduce page table walk latency as
they are co-located in the same region as the data.

Figure~\ref{fig:timeline}c shows the timeline of events
in case of a TLB hit in SpryVM. The virtual address uniquely identifies a
memory region and traverses the NoC and off-chip interconnect to
locate the appropriate memory channel. Then, a simple multiplexer (not
shown in the figure) distinguishes virtual and physical address
requests, and sends the former to the memory channel's TLB. The TLB
holds the translation. From this page frame, we can calculate the
physical address of the data, completing the translation path. Once
this completes, data fetch commences. Unlike the baseline case, note
that spryVM can overlap translation with data fetch because
both the TLB and data are co-located in the same region. As a result, although 
translation itself takes longer time, the overhead of translation in case of a TLB hit 
amounts to the TLB probe latency, as in the conventional case.

With SpryVM, TLB misses occur much less frequently. When they do, as
shown in Figure~\ref{fig:timeline}d, the virtual address
identifies the appropriate memory controller, which can be reached by
traversing the NoC and off-chip interconnects. The key difference is that
the PTE also resides in the same memory region. Therefore, unlike conventional translation, there is no need
to traverse the NoC and off-chip interconnects for page table
lookups. Once the PTE arrives from the local DRAM partition and the
TLB fill operation completes, translation finishes. Then, the memory
controller issues the complete physical address to locate the target
piece of data. When data returns from the DRAM array, it is sent back
to the accelerator by traversing the networks, finishing the data
path.

With SpryVM, page walks are cheaper than in conventional translation
as the time to locate the appropriate memory channel is part of both
the translation path and data path (essentially removing this part of
the cost of address translation). Page table walks do not involve
network communication as the page table entries are all located in the
same memory channel. The longer the latency of the network is with
respect to the DRAM accesses, the lower overhead of page table walks
in SpryVM relative to the conventional architecture. This behavior particularly 
benefits large memory systems, as the need
for scaling out the memory for large-memory machines increase the
average distance to the memory channels, as well as systems with lower DRAM
latencies (e.g., 3D-stacked memories).


\begin{figure*}
	\includegraphics[width=\textwidth]{figures/Fig345.png}
	\caption{Timeline of events of an accelerator's memory reference in conventional translation and and SpryVM .}
	\label{fig:timeline}
\end{figure*}


%\begin{figure*}
%	\includegraphics[width=\textwidth]{figures/time_event_base.pdf}
%	\caption{Timeline of events of an accelerator's memory reference that misses in the TLB on conventional translation.}
%	\label{fig:pagewalk_base}
%\end{figure*}

%\begin{figure*}
%	\includegraphics[width=\textwidth]{figures/time_event_partition_hit.pdf}
%	\caption{Timeline of events of an accelerator's memory reference that hits in the TLB on SpryVM.}
%	\label{fig:pagewalk_partition_hit}
%\end{figure*}

%\begin{figure*}
%	\includegraphics[width=\textwidth]{figures/time_event_partition_miss.pdf}
%	\caption{Timeline of events of an accelerator's memory reference that misses in the TLB on SpryVM.}
%	\label{fig:pagewalk_partition_miss}
%\end{figure*}

 
%In modern page-based VM, a virtual page is mapped on demand into a page frame. Modern systems place no restrictions on the virtual-to-physical mapping; a virtual page can potentially map to any page frame. Though flexible, this fully associative approach places translation on the critical path of every memory access, because a memory access cannot begin until the translation operation finishes.

%One potential way to break translation-memory access serialization is to completely eliminate associativity and enforce a direct mapping between virtual pages and page frames. Since a virtual page can now only map to a single page frame, address translation and data fetch are independent and can proceed in parallel. Conventional wisdom dictates that direct mapping creates an excess of page faults, due to conflicts from multiple virtual pages mapping to the same page frame. However, this is merely intuition, as there exists no study on VM associativity, unlike caches, which are similar in some organizational aspects and for which such a study has existed for three decades~\cite{hill:aspects}. 



%To fill this research void, we employ the 3C model---initially developed for caches~\cite{hill:aspects}---to study VM associativity. In this context, associativity means the number of possible locations (page frames) a given virtual page can map to. This model classifies misses (i.e., page faults) into three categories: conflict misses (when too many active pages map to a fraction of the sets), capacity misses (due to limited memory size), and compulsory misses (upon the first access to a page). 

%To simulate real-world scenarios, we select a set of representative server workloads, summarized in Table~\ref{table:workload}. We include two cloud workloads from CloudSuite~\cite{ferdman:clearing}, Cassandra and Memcached, an online transaction processing (OLTP) workload~\cite{facebook:linkbench}, MySQL, two online analytical processing (OLAP) workloads~\cite{boncz:breaking}, TPC-H and TPC-DS, and a widely-used storage system workload~\cite{dong:optimizing}, RocksDB. We collect long memory traces of several 10s of billions of instructions of the server workloads using Pin~\cite{luk:pin}. We extract the virtual address and address space identifier (ASID) of each memory reference and use it to probe a set-associative memory structure, varying the associativity to observe and classify the misses. A detailed description of our methodology is found in Section~\ref{sec:methodology}.

%\subsection{Single-Process In-Memory}

%Fig.~\ref{fig:miss_ratio} shows results for three single-process workloads: Memcached, RocksDB, and Cassandra. The y-axis breaks down the total misses into the three distinct miss classes. Each category on the x-axis corresponds to the ratio between the size of the physical memory and the size of the application's working set. For example,  $8\times$ indicates that the memory is eight times larger than the application's working set. Similarly, $1/2\times$ means that the application's working set is twice the size of the memory. We collapse results for $8\times$ to $1\times$ because they are similar and visually identical; each case represents a fully in-memory scenario. Furthermore, within each working set category, the x-axis sweeps through different VM associativities, from direct-mapped to 32-way associative. 



%Even with a simple direct-mapped translation, compulsory misses represent $99.9\%$ of all the misses. There are no capacity misses as the working set fully fits in memory. Conflict misses are scarce. For example, Memcached using direct-mapped translation achieves a conflict miss rate in the order of one miss per $10^{8}$ memory accesses. Using $2$ ways removes all the conflicts for the $8\times$, $4\times$, and $2\times$ cases, while $4$ ways are required for the $1\times$ case (where the memory size is equal to the size of the working set). For in-memory scenarios, page conflicts arise because the virtual address space of server applications is particularly sparse; there are many virtual segments scattered all over the address space. For example, Java processes exhibit many virtual segments due to the dynamic nature of the JVM. This is best observed in the case of Cassandra, which exhibits direct-mapped conflict miss rates in the order of one miss per $10^{7}$ and $10^{6}$ memory accesses for the $8\times$--$2\times$ cases and the $1\times$ case respectively. However, conflict misses drop rapidly as associativity increases and using 4 ways removes all conflicts for the $4\times$ and $2\times$ cases, while virtually eliminating conflicts for the $1\times$ case. The observation that conflict misses drop rapidly with associativity has also been shown for caches~\cite{hill:aspects, cantin:cache}. We elide the results for TPC-H, TPC-DS, and MySQL as they follow identical trends.

%\subsection{Single-Process Out-of-Memory}

%In contrast to in-memory scenarios, when the working sets do not fit in memory ($1/2\times$, $1/4\times$, $1/8\times$ cases), capacity misses grow with the working set size, while the fraction of compulsory misses drops. Although conflict misses are more significant than in the in-memory scenarios, conflicts drop sharply after 2-4 ways. In the worst case, 16 ways are required to drive conflict misses down to $\sim1\%$ of all the misses. Note that Cassandra has an active working set that fits in a memory of $1/4\times$ the data size, and capacity misses start to rise at the $1/8\times$ case and beyond. Fundamentally, all these results corroborate the seminal work on caches~\cite{hill:aspects, cantin:cache}. 

%\subsection{Multi-programming In-Memory}
%Fig.~\ref{fig:miss_ratio_procs} presents results for multi-programming. We take each of the three workloads, Memcached, RocksDB, and Cassandra, and increase the number of processes, keeping the overall working set size the same with respect to the single process scenarios. For example, for the $1\times$ case in Fig.~\ref{fig:miss_ratio}, the working set of a single process equals the size of the memory. In Fig.~\ref{fig:miss_ratio_procs}, for two processes sharing the same physical memory, the per-process working set is half the size of the physical memory. Similarly, with four processes, per-process working sets consume a quarter of the physical memory. We thus guarantee that only the increase in the number of processes has an impact on the associativity. 

%For in-memory scenarios ($8\times$--$1\times$ cases), once the associativity equals the number of processes, compulsory misses represent $99.9\%$ of all the misses for all the workloads. Increasing the associativity further makes conflicts virtually disappear after a few additional ways. For instance, for Memcached, with 8, 16, and 32 ways, the conflicts are in the order of a single miss per $10^{9}$ accesses for 2, 4, and 8 processes, respectively. Cassandra requires 4, 8, and 8 ways to achieve a miss conflict rate of one miss per $10^{9}$ accesses for 2, 4, and 8 processes, respectively. Again, the trends are identical for TPC-H, TPC-DS, and MySQL. Overall, conflict misses become virtually zero with a few ways (i.e., 2-4) per process.

%%%% THINK WHETHER TO ADD THIS
%Although Fig.~\ref{fig:miss_ratio} and \ref{fig:miss_ratio_procs} show results that use the standard method of computing the index by using the least-significant bits of the VPN, we also studied an alternative proposal whose strength is distributing address more evenly over sets~\cite{qureshi:fundamental}. It did not reduce conflicts significantly due to the fact that page conflicts arise when many pages map to the same memory set, even when assuming a uniform page distribution. Changing the index function uniformly changes the distribution, and therefore does not mitigate the issue. Once again this behavior corroborates prior work on caches~\cite{hill:aspects}.

%\subsection{Multi-programming Out-of-Memory}

%For the cases where the working sets do not fit in memory, the trends are similar to the single-process scenarios. Capacity misses become more significant as the working set sizes grow, making conflict and compulsory misses less important. Similar to the in-memory case, once the associativity equals the number of processes, the fraction of conflict misses matches the single-process results. Conflicts drop rapidly as in the other cases and with 4 ways per process, conflict misses remain within $1\%$ of the total misses for all the workloads. The results for TPC-H, TPC-DS, and MySQL are identical.

%\subsection{Observations \& Implications}

%Our results show that fully associative VM is unnecessary for many modern server workloads. This observation holds across scenarios that span single process, multi-programming, in-memory and out-of-memory working sets. Compulsory and capacity misses dominate, and conflict misses, which associativity alleviates, drop rapidly as the associativity increases. Specifically, for in-memory scenarios, compulsory misses dominate when associativity equals the number of processes. For out-of-the-memory scenarios, capacity misses dominate and become more important as working set sizes increase. In this case, conflict misses become scarce once associativity matches the number of processes, and virtually disappear after a few extra ways. These trends match prior literature on caches~\cite{hill:aspects, cantin:cache, hill:case}---just as set-associative (or direct-mapped) caches can often provide nearly all the benefits of full associativity with faster access times, set-associative VM can provide nearly all the benefits of full associativity with faster translation. 

%To put the associativity requirements into perspective, a commodity server today integrates $256$GB of memory~\cite{ning:open}. Assuming $4$KB pages, fully associative VM provides $64M$ ways. As there are around $100$ processes concurrently running after booting~\cite{ahn:revising}, even when assuming a system with 128 processes and an associativity of 16 per process---which is extreme as we have seen that an associativity of 2--4 per process is sufficient---the total associativity requirements does not exceed $2K$ ways. Even for a commodity server, the $2K$ requirement is $32K\times$ less than what full associativity provides. A server with a larger memory capacity (e.g., large NUMA machines~\cite{hp:hp, huawei:kunlun}) or with emerging non-volatile memories (e.g., 3D XPoint~\cite{3dxpoint}) would supply even more ways, widening the gap between the required and provided associativity. 



